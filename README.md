#  HybridRAG-U9-OptimizedSearch 

Traditional RAG pipelines often struggle to retrieve the most relevant document pieces because splitting large documents into smaller chunks can disconnect important context from the broader content. This can lead to less accurate or incomplete answers.

In this project, we tackle that limitation by attaching **document-level context** to each chunk before indexing, and by combining **semantic (vector) retrieval** with **keyword-based (BM25)** search. The results are further refined through a **reranking** stage for improved precision.

---

## System Overview

**Main Components**

1. A large language model (e.g., `GPT-4o-mini` or `Llama 3.2`).
2. Embedding generator — either OpenAI’s `text-embedding-3-small` or an Ollama embedding model.
3. A **Chroma** vector database for semantic similarity search.
4. A **BM25** keyword index built from TF-IDF vectors.
5. A **hybrid retrieval** module combining both search strategies.
6. A reranker model (`BAAI/bge-reranker-v2-m3`) for fine-grained relevance scoring.
7. **LangChain** is used as the orchestration framework.

---

## Workflow Breakdown

### 1. Data Preparation

* Both JSON and PDF documents are ingested along with their metadata (e.g., titles, page numbers, sources).
* Each document is segmented into smaller text units using a `Recursive Character Text Splitter` or similar strategy.

### 2. Adding Document Context

* Every chunk is enriched with a short, document-level context summary generated by an LLM (such as GPT-4o-mini or Llama 3.2).
* The prompt requests a brief (3–4 sentence) explanation of how the chunk fits into the broader document.
* This context is prepended to the chunk’s text before embedding, giving the retriever a stronger sense of meaning and relation.

### 3. Vector Embedding and Semantic Indexing

* The contextualized chunks are converted into dense vectors using an embedding model (`text-embedding-3-small` or another open model).
* These vectors are stored in a **Chroma** database using cosine similarity as the distance metric for retrieval.

### 4. BM25 Keyword Index

* TF-IDF representations of all chunks are stored in a **BM25** index to support exact keyword matching and boost precision for term-heavy queries.

### 5. Hybrid Search via Reciprocal Rank Fusion

* Both the BM25 and vector-based searches are performed simultaneously.
* Each retriever returns its top results (typically top-5), which are then normalized to a common score range (0–1).
* Scores are merged using a weighted approach so that overlapping results gain higher priority.

### 6. Reranking for Final Selection

* The combined search results are reranked with `BAAI/bge-reranker-v2-m3`, a cross-encoder model that evaluates the semantic relationship between each document and the query.
* Only the top few (default 5) documents are returned after reranking to ensure optimal relevance.

### 7. Building the RAG Pipeline

* A custom RAG prompt template is used, where user questions and retrieved contexts are dynamically merged before being passed to the LLM.
* The final generation step produces an informed, contextually grounded answer.

---

This setup enables more contextually aware responses, minimizes hallucinations, and significantly enhances retrieval accuracy in complex document-based QA tasks.

---

